{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMH0uJak54jqKj2LrDU+j5c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Talkeqd5__pQ","executionInfo":{"status":"error","timestamp":1742468261807,"user_tz":0,"elapsed":153730,"user":{"displayName":"Tomas Smitas","userId":"05357071764865763053"}},"outputId":"d1f1ec95-0bea-48b8-ef90-9f3fc6d130cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","--2025-03-20 10:55:10--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2025-03-20 10:55:11--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2025-03-20 10:55:11--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip         94%[=================>  ] 775.15M  5.07MB/s    eta 9s     ^C\n","Archive:  glove.6B.zip\n","  End-of-central-directory signature not found.  Either this file is not\n","  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n","  latter case the central directory and zipfile comment will be found on\n","  the last disk(s) of this archive.\n","unzip:  cannot find zipfile directory in one of glove.6B.zip or\n","        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'glove.6B.50d.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-fd9cce1b7b4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.50d.txt\"\u001b[0m  \u001b[0;31m# Path to the GloVe file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"]}],"source":["# Step 1: Install required libraries\n","!pip install numpy tensorflow scikit-learn\n","\n","# Step 2: Import libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Step 3: Download and load GloVe embeddings\n","# Download GloVe embeddings (run this only once)\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# Load GloVe embeddings\n","glove_path = \"glove.6B.50d.txt\"  # Path to the GloVe file\n","embeddings_index = {}\n","with open(glove_path, encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","print(f\"Loaded {len(embeddings_index)} word vectors.\")\n","\n","# Step 4: Prepare the data\n","# Define positive and negative words\n","positive_words = [\n","    \"happy\", \"joy\", \"love\", \"peace\", \"success\", \"brilliant\", \"amazing\", \"wonderful\", \"fantastic\", \"excellent\",\n","    \"delight\", \"gratitude\", \"hope\", \"optimism\", \"victory\", \"harmony\", \"laughter\", \"smile\", \"thrilled\", \"blessed\",\n","    \"ecstatic\", \"cheerful\", \"jubilant\", \"euphoric\", \"content\", \"serene\", \"blissful\", \"radiant\", \"vibrant\", \"inspired\",\n","    \"confident\", \"proud\", \"energized\", \"enthusiastic\", \"passionate\", \"fulfilled\", \"glorious\", \"magnificent\", \"stellar\", \"superb\",\n","    \"outstanding\", \"remarkable\", \"splendid\", \"phenomenal\", \"marvelous\", \"incredible\", \"extraordinary\", \"fabulous\", \"divine\", \"heavenly\"\n","]\n","negative_words = [\n","    \"sad\", \"anger\", \"hate\", \"pain\", \"failure\", \"terrible\", \"awful\", \"horrible\", \"disgust\", \"despair\",\n","    \"misery\", \"regret\", \"fear\", \"anxiety\", \"defeat\", \"conflict\", \"cry\", \"frustration\", \"disappointment\", \"suffering\",\n","    \"grief\", \"sorrow\", \"heartbreak\", \"loneliness\", \"isolation\", \"rejection\", \"abandonment\", \"betrayal\", \"humiliation\", \"shame\",\n","    \"guilt\", \"remorse\", \"doubt\", \"insecurity\", \"jealousy\", \"envy\", \"resentment\", \"bitterness\", \"hostility\", \"rage\"\n","]\n","\n","# Combine words and assign labels\n","words = positive_words + negative_words\n","labels = [1] * len(positive_words) + [0] * len(negative_words)  # 1 = positive, 0 = negative\n","\n","# Convert words to GloVe embeddings\n","def word_to_glove(word, embeddings_index):\n","    return embeddings_index.get(word, np.zeros(50))  # 50-dimensional GloVe embeddings\n","\n","X = np.array([word_to_glove(word, embeddings_index) for word in words])\n","y = np.array(labels)\n","\n","# Split data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 5: Build the model\n","model = Sequential([\n","    Dense(16, activation='relu', input_shape=(50,)),  # Input shape is 50 (GloVe dimensionality)\n","    Dense(1, activation='sigmoid')  # Output layer\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Print model summary\n","model.summary()\n","\n","# Step 6: Train the model\n","history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n","\n","# Step 7: Evaluate the model\n","loss, accuracy = model.evaluate(X_val, y_val)\n","print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Step 8: Predict on unseen words\n","def predict_word_sentiment(word, model, embeddings_index):\n","    vector = word_to_glove(word, embeddings_index)\n","    prediction = model.predict(np.array([vector]))\n","    return \"Positive\" if prediction > 0.5 else \"Negative\"\n","\n","# Test the model on unseen words\n","unseen_words = [\"bad\", \"good\", \"hydrated\", \"dehydrated\", \"moon\"]\n","for word in unseen_words:\n","    result = predict_word_sentiment(word, model, embeddings_index)\n","    print(f\"{word}: {result}\")"]},{"cell_type":"code","source":["# List of unseen words to test\n","unseen_words = [\"bad\", \"good\", \"hydrated\", \"dehydrated\", \"moon\"]\n","\n","# Function to predict sentiment of a word\n","def predict_word_sentiment(word, model, word_to_index):\n","    if word not in word_to_index:\n","        return \"Word not in vocabulary\"\n","    vector = one_hot_encode(word, word_to_index)\n","    prediction = model.predict(np.array([vector]))\n","    return \"Positive\" if prediction > 0.5 else \"Negative\"\n","\n","# Test each unseen word\n","for word in unseen_words:\n","    result = predict_word_sentiment(word, model, word_to_index)\n","    print(f\"{word}: {result}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26ucohxcIYcT","executionInfo":{"status":"ok","timestamp":1742467930798,"user_tz":0,"elapsed":15,"user":{"displayName":"Tomas Smitas","userId":"05357071764865763053"}},"outputId":"58d6d322-d446-4c66-81e3-e43acdba1e20"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["bad: Word not in vocabulary\n","good: Word not in vocabulary\n","hydrated: Word not in vocabulary\n","dehydrated: Word not in vocabulary\n","moon: Word not in vocabulary\n"]}]}]}